{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***NLP - NATURAL LANGUAGE PROCESSING***"
      ],
      "metadata": {
        "id": "68l9uv3MZR1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***1.Tokenization***"
      ],
      "metadata": {
        "id": "Ox2Sr8BYdFZ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMYPZzQOF4sx",
        "outputId": "92817b9a-aad7-45b8-aab9-a1fc17389b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpRJWUWtGMcd",
        "outputId": "dc77872e-bb63-4255-9046-10f589d65534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenizer:  ['Hello', ',', 'This', 'is', 'Raj.Nice', 'to', 'meet', 'you', '!', 'Have', 'a', 'good', 'day1']\n",
            "Sentence tokenizer:  ['Hello,This is Raj.Nice to meet you!Have a good day1']\n"
          ]
        }
      ],
      "source": [
        "text =  'Hello,This is Raj.Nice to meet you!Have a good day1'\n",
        "word = word_tokenize(text)\n",
        "sentence = sent_tokenize(text)\n",
        "print('Word tokenizer: ',word)\n",
        "print('Sentence tokenizer: ',sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***2.Stopwords Removal***"
      ],
      "metadata": {
        "id": "1xuiCF4ydQKv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx5jSMmBGcWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebc3789-053c-4e5b-f862-ef0d158529e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Filtered Words:\", filtered_words)"
      ],
      "metadata": {
        "id": "qwcGUImXdodd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04e7eca-ef43-4232-a579-60a47b163608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['Hello', ',', 'This', 'is', 'Raj.Nice', 'to', 'meet', 'you', '!', 'Have', 'a', 'good', 'day1']\n",
            "Filtered Words: ['Hello', ',', 'Raj.Nice', 'meet', '!', 'good', 'day1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***3.Stemming***"
      ],
      "metadata": {
        "id": "bP8OF3cAfHQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "5bNwDvyofOIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "words = word_tokenize(text)\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"Original Words:\", word)\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDhcmwXXfTd2",
        "outputId": "653019f1-d5e6-42e7-9b4f-1cf843143e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['Hello', ',', 'This', 'is', 'Raj.Nice', 'to', 'meet', 'you', '!', 'Have', 'a', 'good', 'day1']\n",
            "Stemmed Words: ['hello', ',', 'thi', 'is', 'raj.nic', 'to', 'meet', 'you', '!', 'have', 'a', 'good', 'day1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***4.Lemmatization***"
      ],
      "metadata": {
        "id": "UwuyM0BYfcma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCwh4akWfVZP",
        "outputId": "30cc3edc-d1cc-4569-a1f3-c485cc752eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkL5rWc2fqLP",
        "outputId": "d3c304a9-e323-44e7-a587-82b8f78c2bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['Hello', ',', 'This', 'is', 'Raj.Nice', 'to', 'meet', 'you', '!', 'Have', 'a', 'good', 'day1']\n",
            "Lemmatized Words: ['Hello', ',', 'This', 'is', 'Raj.Nice', 'to', 'meet', 'you', '!', 'Have', 'a', 'good', 'day1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***5.Part of Speech (POS)***"
      ],
      "metadata": {
        "id": "P_skrNN6fybg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kP-EG8rf5ep",
        "outputId": "1dd348ad-bd6b-4976-b429-c8b348eb0731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = nltk.pos_tag(words)\n",
        "print(\"POS Tagging Result:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FC3IinLf_ea",
        "outputId": "761e0a5c-f04d-4860-98cd-2ac9563bb14f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging Result: [('Hello', 'NNP'), (',', ','), ('This', 'DT'), ('is', 'VBZ'), ('Raj.Nice', 'NNP'), ('to', 'TO'), ('meet', 'VB'), ('you', 'PRP'), ('!', '.'), ('Have', 'VB'), ('a', 'DT'), ('good', 'JJ'), ('day1', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***6. Named Entity Recognition (NER)***"
      ],
      "metadata": {
        "id": "BHmWJA2kgIAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Mp9sPrSgMtH",
        "outputId": "eb5779ad-6e67-47ad-b3d9-eb8e1e872e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "named_entities = nltk.ne_chunk(pos_tags)\n",
        "print(\"Named Entities Tree:\")\n",
        "print(named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTc83zmjgSzH",
        "outputId": "352fc1ff-89af-438e-9999-1e9dba1e5425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities Tree:\n",
            "(S\n",
            "  (GPE Hello/NNP)\n",
            "  ,/,\n",
            "  This/DT\n",
            "  is/VBZ\n",
            "  Raj.Nice/NNP\n",
            "  to/TO\n",
            "  meet/VB\n",
            "  you/PRP\n",
            "  !/.\n",
            "  Have/VB\n",
            "  a/DT\n",
            "  good/JJ\n",
            "  day1/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***7.Bag of Words (BoW)***"
      ],
      "metadata": {
        "id": "p9mklllDgcHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "1eO_6xPEgVdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences =['I like ml','ml is interesting','I enjoy learning ml']\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = (vectorizer.fit_transform(sentences))\n",
        "print(\"Bag of Words Matrix:\\n\", bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--5hF1lIhQDx",
        "outputId": "2e19064f-c856-428d-ff9a-836b4df5c016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Matrix:\n",
            " [[0 0 0 0 1 1]\n",
            " [0 1 1 0 0 1]\n",
            " [1 0 0 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***8.TF-IDF***"
      ],
      "metadata": {
        "id": "x9uKqsbOjX-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "T7DGHdWMjdaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee4e6lrrjogs",
        "outputId": "0f5f87eb-b00c-4e32-f562-5f3d725d081a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['enjoy' 'interesting' 'is' 'learning' 'like' 'ml']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.         0.861037   0.50854232]\n",
            " [0.         0.65249088 0.65249088 0.         0.         0.38537163]\n",
            " [0.65249088 0.         0.         0.65249088 0.         0.38537163]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***9.Word Embeddings (Word2Vec & GloVe)***"
      ],
      "metadata": {
        "id": "eHL8TQOLkPxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDRC7wGFmfSk",
        "outputId": "551c2a7e-7e5a-42b2-a7bd-e17f339aee45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [\n",
        "    ['i', 'love', 'nlp'],\n",
        "    ['nlp', 'is', 'fun'],\n",
        "    ['machine', 'learning', 'is', 'awesome'],\n",
        "    ['deep', 'learning', 'is', 'part', 'of', 'nlp']\n",
        "]\n",
        "\n",
        "model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, workers=1)\n",
        "\n",
        "print(\"Vector for 'nlp':\", model.wv['nlp'])\n",
        "print(\"Similarity between 'nlp' and 'learning':\", model.wv.similarity('nlp', 'learning'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyKnyGt7oHyj",
        "outputId": "2bbf1a50-88f9-48cc-9b89-1af358b6d745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'nlp': [ 0.07380505 -0.01533471 -0.04536613  0.06554051 -0.0486016  -0.01816018\n",
            "  0.0287658   0.00991874 -0.08285215 -0.09448818]\n",
            "Similarity between 'nlp' and 'learning': 0.32937223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***10.Text Preprocessing Pipeline***"
      ],
      "metadata": {
        "id": "D7PCgpOIocXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "AFWECnxop1gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I'm senbagarajan, a first year Computer Science student.\"\n",
        "text = text.lower()\n",
        "words = word_tokenize(text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "print(\"Preprocessed words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CuGYn68ojN-",
        "outputId": "3079e6d2-049b-41c4-96d8-3a579c6dda93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed words: ['senbagarajan', 'first', 'year', 'computer', 'science', 'student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***11.Sentiment Analysis***"
      ],
      "metadata": {
        "id": "T5R3gOIfqL0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm_wgRZlqSii",
        "outputId": "db75005b-eaa1-40ac-d4ab-43cbbfff41a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "ss2L2bSsqXIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"I'm happy.\"\n",
        "sentence2 = \"I'm sad.\"\n",
        "sentence3 = \"I'm ok\"\n",
        "blob1 = TextBlob(sentence1)\n",
        "blob2 = TextBlob(sentence2)\n",
        "blob3 = TextBlob(sentence3)\n",
        "sentiment1 = blob1.sentiment.polarity\n",
        "sentiment2 = blob2.sentiment.polarity\n",
        "sentiment3 = blob3.sentiment.polarity\n",
        "print(\"Sentiment of sentence1:\", sentiment1)\n",
        "print(\"Sentiment of sentence2:\", sentiment2)\n",
        "print(\"Sentiment of sentence3:\", sentiment3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PCSuczFqcdt",
        "outputId": "ed0e17fb-7a48-450b-8ffc-3a360d4afd44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment of sentence1: 0.8\n",
            "Sentiment of sentence2: -0.5\n",
            "Sentiment of sentence3: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***12.Text Classification***"
      ],
      "metadata": {
        "id": "YxSx5JMCrVYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n"
      ],
      "metadata": {
        "id": "AoN9wbGdsNGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "texts = [\"Free money!!!\", \"Hi, how are you?\", \"Win a brand new car!\", \"Let's catch up tomorrow.\"]\n",
        "labels = [\"spam\", \"ham\", \"spam\", \"ham\"]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X, labels)\n",
        "\n",
        "test_text = [\"free cashback\"]\n",
        "X_test = vectorizer.transform(test_text)\n",
        "print(\"Prediction:\", model.predict(X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtUE6kxzrbp3",
        "outputId": "149451b3-d90b-45c6-f203-3cf18cec1b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: ['spam']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***13.Language Translation***"
      ],
      "metadata": {
        "id": "FwwUXZKVs8ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mztR-zYHuD6K",
        "outputId": "a493580d-9fc4-4352-ca2c-947ba0964b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.11/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.11/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.7.14)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.11/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.11/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.11/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "\n",
        "translator = Translator()\n",
        "result = translator.translate(\"Good morning, have a nice day!\", dest='ta')\n",
        "\n",
        "print(\"Translated to Tamil:\", result.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXhkQ_UmuIbw",
        "outputId": "6ee9ccd6-a3cb-4f94-918f-0b0e1b676926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated to Tamil: காலை வணக்கம், ஒரு நல்ல நாள்!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***14.Text Generation***"
      ],
      "metadata": {
        "id": "2y2kTJZhu7Q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTeD2s-jvEb2",
        "outputId": "7efa6a71-f3db-4ad7-8992-33b12000aa3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "output = generator(\"Once upon a time in a village,\", max_length=50, num_return_sequences=1)\n",
        "print(\"Generated Text:\", output[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxexaDI2vQWR",
        "outputId": "9b77f6ce-eee3-49b7-a44d-0c64eb647697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: Once upon a time in a village, a woman's eyes glittered with a light of red. She looked down on the young woman's face.\n",
            "\n",
            "\"You look a little strange looking at you, you're quite young, you look like such a good girl.\"\n",
            "\n",
            "A girl was looking down at her.\n",
            "\n",
            "\"You're so cute, you're so cute, you look so cute.\"\n",
            "\n",
            "Her eyes looked into the girl's.\n",
            "\n",
            "\"I'm so happy, I'm so happy, I'm so happy, you're such a good girl.\"\n",
            "\n",
            "A girl's eyes looked into the girl's.\n",
            "\n",
            "\"You're so cute, you look so cute, you look so cute.\"\n",
            "\n",
            "A girl's eyes looked into the girl's.\n",
            "\n",
            "\"You look such a good girl. I'm so happy, I'm so happy, I'm so happy, you're such a good girl.\"\n",
            "\n",
            "A girl's eyes looked into the girl's.\n",
            "\n",
            "\"You like me, you like me, you like me, you like me.\"\n",
            "\n",
            "A girl's eyes looked into the girl's.\n",
            "\n",
            "\"You're so cute, you look so cute, you look so cute.\"\n",
            "\n",
            "A girl's eyes looked into the girl's\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}